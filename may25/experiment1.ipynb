{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Code Smell Detection Using Large Language Models\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Code smells - patterns in source code that indicate potential design problems - are critical indicators of software quality issues. Traditionally, identifying code smells has relied on:\n",
    "\n",
    "1. Manual code reviews (time-consuming and subjective)\n",
    "2. Static analysis tools (limited to predefined patterns)\n",
    "3. Software metrics (often producing false positives)\n",
    "\n",
    "These approaches either require significant human effort or lack the contextual understanding needed to identify subtle design issues.\n",
    "\n",
    "## How Generative AI Solves This Problem\n",
    "\n",
    "This notebook demonstrates how Large Language Models (LLMs) can transform code smell detection by:\n",
    "\n",
    "- **Contextual understanding**: Analyzing code with deep semantic understanding rather than just pattern matching\n",
    "- **Knowledge integration**: Leveraging structured knowledge about different code smell types and refactoring solutions\n",
    "- **Natural language reasoning**: Providing detailed explanations and actionable recommendations in human-readable format\n",
    "\n",
    "## What This Notebook Demonstrates\n",
    "\n",
    "This end-to-end implementation shows how to:\n",
    "\n",
    "1. Build a knowledge base of code smells using vector embeddings and DeepLake\n",
    "2. Create a retrieval-augmented generation (RAG) system to provide context about code smells\n",
    "3. Analyze Java code files to detect multiple types of code smells\n",
    "4. Generate structured output with smell locations, severity, and refactoring suggestions\n",
    "5. Evaluate detection quality against manually labeled data\n",
    "\n",
    "The approach provides a practical showcase of how generative AI can be applied to enhance software development practices through automated code quality assessment.\n",
    "\n",
    "**Now, let's begin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*install all needed dependencies:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:44.309246Z",
     "iopub.status.busy": "2025-04-21T04:12:44.308907Z",
     "iopub.status.idle": "2025-04-21T04:12:49.180332Z",
     "shell.execute_reply": "2025-04-21T04:12:49.179219Z",
     "shell.execute_reply.started": "2025-04-21T04:12:44.309221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: lz4 in /Users/havriil.pietukhin/uni/masterThesis/.venv/lib/python3.9/site-packages (4.4.4)\n",
      "Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-11.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U \"langchain-google-genai\" \"deeplake\" \"langchain\" \"langchain-text-splitters\" \"langchain-community\" \"tiktoken\" \"google-ai-generativelanguage==0.6.15\" \"deeplake[enterprise]<4.0.0\"\n",
    "%pip install pillow lz4 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now initialize deep lake vectore store and store structured info about code smells in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use 2 repos as a data sources:\n",
    "\n",
    "* pixel-dungeon - as an example of software that contains numerous code smells.\n",
    "* smells - a comprehensive classification of code smells. we will use structured information about code smells from this repository in our DeepLake database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:49.182543Z",
     "iopub.status.busy": "2025-04-21T04:12:49.182281Z",
     "iopub.status.idle": "2025-04-21T04:12:49.439865Z",
     "shell.execute_reply": "2025-04-21T04:12:49.439005Z",
     "shell.execute_reply.started": "2025-04-21T04:12:49.182518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pixel-dungeon'...\n",
      "remote: Enumerating objects: 2263, done.\u001b[K\n",
      "remote: Counting objects: 100% (1805/1805), done.\u001b[K\n",
      "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
      "remote: Total 2263 (delta 1558), reused 1339 (delta 1339), pack-reused 458 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2263/2263), 4.19 MiB | 17.81 MiB/s, done.\n",
      "Resolving deltas: 100% (1656/1656), done.\n",
      "Cloning into 'smells'...\n",
      "remote: Enumerating objects: 1045, done.\u001b[K\n",
      "remote: Counting objects: 100% (366/366), done.\u001b[K\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
      "remote: Total 1045 (delta 151), reused 331 (delta 142), pack-reused 679 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1045/1045), 4.16 MiB | 5.39 MiB/s, done.\n",
      "Resolving deltas: 100% (396/396), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/watabou/pixel-dungeon.git\n",
    "!git clone https://github.com/Luzkan/smells.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all files with structured info about code smells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:49.441348Z",
     "iopub.status.busy": "2025-04-21T04:12:49.441030Z",
     "iopub.status.idle": "2025-04-21T04:12:49.447247Z",
     "shell.execute_reply": "2025-04-21T04:12:49.446321Z",
     "shell.execute_reply.started": "2025-04-21T04:12:49.441321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/havriil.pietukhin/uni/masterThesis/.venv/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.2.1) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "import deeplake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a Kaggle secret named GOOGLE_API_KEY.\n",
    "\n",
    "If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.\n",
    "\n",
    "To make the key available through Kaggle secrets, choose Secrets from the Add-ons menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:49.449292Z",
     "iopub.status.busy": "2025-04-21T04:12:49.449055Z",
     "iopub.status.idle": "2025-04-21T04:12:49.546729Z",
     "shell.execute_reply": "2025-04-21T04:12:49.546037Z",
     "shell.execute_reply.started": "2025-04-21T04:12:49.449266Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_secrets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkaggle_secrets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m api_key\u001b[38;5;241m=\u001b[39mUserSecretsClient()\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:49.547636Z",
     "iopub.status.busy": "2025-04-21T04:12:49.547372Z",
     "iopub.status.idle": "2025-04-21T04:12:49.554060Z",
     "shell.execute_reply": "2025-04-21T04:12:49.553327Z",
     "shell.execute_reply.started": "2025-04-21T04:12:49.547618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "smells_match = \"smells/content/smells/**/*.md\"\n",
    "all_smells = glob(smells_match, recursive=True)\n",
    "# Print the first 10 files\n",
    "print(f\"First 5 of {len(all_smells)} markdown files:\")\n",
    "for file in all_smells[:5]:\n",
    "    print(f\"  {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file with a matching path will be loaded and split by RecursiveCharacterTextSplitter. only Markdown files with structured content will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:49.555112Z",
     "iopub.status.busy": "2025-04-21T04:12:49.554842Z",
     "iopub.status.idle": "2025-04-21T04:12:49.571207Z",
     "shell.execute_reply": "2025-04-21T04:12:49.570201Z",
     "shell.execute_reply.started": "2025-04-21T04:12:49.555084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# common seperators used for Python files\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.MARKDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:49.572620Z",
     "iopub.status.busy": "2025-04-21T04:12:49.572396Z",
     "iopub.status.idle": "2025-04-21T04:12:53.017736Z",
     "shell.execute_reply": "2025-04-21T04:12:53.016592Z",
     "shell.execute_reply.started": "2025-04-21T04:12:49.572599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:53.019468Z",
     "iopub.status.busy": "2025-04-21T04:12:53.019170Z",
     "iopub.status.idle": "2025-04-21T04:12:53.054188Z",
     "shell.execute_reply": "2025-04-21T04:12:53.053373Z",
     "shell.execute_reply.started": "2025-04-21T04:12:53.019441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Define headers that match your code smell documentation structure\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Title\"),\n",
    "    (\"##\", \"Section\"),\n",
    "    (\"###\", \"Subsection\")\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for file in all_smells:\n",
    "    try:\n",
    "        # First load the file as text\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract the main content (everything after the frontmatter)\n",
    "        if '---' in content:\n",
    "            # Find the second occurrence of '---' which ends the frontmatter\n",
    "            parts = content.split('---', 2)\n",
    "            if len(parts) >= 3:\n",
    "                markdown_content = '---' + parts[2]  # Keep the separator for proper markdown parsing\n",
    "            else:\n",
    "                markdown_content = content\n",
    "        else:\n",
    "            markdown_content = content\n",
    "            \n",
    "        # Split by markdown headers\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "        header_splits = markdown_splitter.split_text(markdown_content)\n",
    "        \n",
    "        # Add source file info to metadata\n",
    "        for doc in header_splits:\n",
    "            doc.metadata['source'] = file\n",
    "        \n",
    "        # Add to collection\n",
    "        docs.extend(header_splits)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "print(f\"Created {len(docs)} chunks from {len(all_smells)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:53.055381Z",
     "iopub.status.busy": "2025-04-21T04:12:53.055035Z",
     "iopub.status.idle": "2025-04-21T04:12:53.059560Z",
     "shell.execute_reply": "2025-04-21T04:12:53.058839Z",
     "shell.execute_reply.started": "2025-04-21T04:12:53.055356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define path to database\n",
    "dataset_path = 'mem://deeplake/smells'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:53.063671Z",
     "iopub.status.busy": "2025-04-21T04:12:53.062806Z",
     "iopub.status.idle": "2025-04-21T04:12:53.078015Z",
     "shell.execute_reply": "2025-04-21T04:12:53.077165Z",
     "shell.execute_reply.started": "2025-04-21T04:12:53.063641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define the embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:53.079504Z",
     "iopub.status.busy": "2025-04-21T04:12:53.079166Z",
     "iopub.status.idle": "2025-04-21T04:12:58.318953Z",
     "shell.execute_reply": "2025-04-21T04:12:58.318051Z",
     "shell.execute_reply.started": "2025-04-21T04:12:53.079483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "smell_db = DeepLake.from_documents(docs, embeddings, dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:58.320342Z",
     "iopub.status.busy": "2025-04-21T04:12:58.320099Z",
     "iopub.status.idle": "2025-04-21T04:12:58.324688Z",
     "shell.execute_reply": "2025-04-21T04:12:58.324035Z",
     "shell.execute_reply.started": "2025-04-21T04:12:58.320323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever = smell_db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 20 # number of documents to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:58.325899Z",
     "iopub.status.busy": "2025-04-21T04:12:58.325600Z",
     "iopub.status.idle": "2025-04-21T04:12:58.343079Z",
     "shell.execute_reply": "2025-04-21T04:12:58.342348Z",
     "shell.execute_reply.started": "2025-04-21T04:12:58.325871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define the chat model\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:58.344330Z",
     "iopub.status.busy": "2025-04-21T04:12:58.344040Z",
     "iopub.status.idle": "2025-04-21T04:12:58.360552Z",
     "shell.execute_reply": "2025-04-21T04:12:58.359716Z",
     "shell.execute_reply.started": "2025-04-21T04:12:58.344307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_llm(llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:58.361637Z",
     "iopub.status.busy": "2025-04-21T04:12:58.361393Z",
     "iopub.status.idle": "2025-04-21T04:12:58.375175Z",
     "shell.execute_reply": "2025-04-21T04:12:58.374280Z",
     "shell.execute_reply.started": "2025-04-21T04:12:58.361608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a helper function for calling retrival chain\n",
    "def call_qa_chain(prompt):\n",
    "  response = qa.invoke(prompt)\n",
    "  display(Markdown(response[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:12:58.376538Z",
     "iopub.status.busy": "2025-04-21T04:12:58.376144Z",
     "iopub.status.idle": "2025-04-21T04:13:01.054877Z",
     "shell.execute_reply": "2025-04-21T04:13:01.054035Z",
     "shell.execute_reply.started": "2025-04-21T04:12:58.376510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "call_qa_chain(\"List a recommendations on how to get rid of 'God class' smell, rating from more simple to more sophisticated strategies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:01.056074Z",
     "iopub.status.busy": "2025-04-21T04:13:01.055791Z",
     "iopub.status.idle": "2025-04-21T04:13:01.065769Z",
     "shell.execute_reply": "2025-04-21T04:13:01.065016Z",
     "shell.execute_reply.started": "2025-04-21T04:13:01.056047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import enum\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Get all markdown files in the content/smells directory\n",
    "smell_files = all_smells\n",
    "\n",
    "# Create enum class dynamically\n",
    "def create_code_smell_enum():\n",
    "    # Process each filename to create enum-friendly names\n",
    "    enum_entries = {}\n",
    "    \n",
    "    for file_path in smell_files:\n",
    "        # Extract filename without extension\n",
    "        filename = Path(file_path).stem\n",
    "        \n",
    "        # Convert kebab-case to UPPER_SNAKE_CASE for enum names\n",
    "        enum_name = filename.replace('-', '_').upper()\n",
    "        \n",
    "        # Use the original filename (without extension) as the value\n",
    "        enum_entries[enum_name] = filename\n",
    "    \n",
    "    # Create and return the Enum class\n",
    "    return enum.Enum('CodeSmell', enum_entries)\n",
    "\n",
    "# Create the enum\n",
    "CodeSmell = create_code_smell_enum()\n",
    "\n",
    "# Display the enum members\n",
    "print(f\"Created enum with {len(CodeSmell)} code smells:\")\n",
    "for smell in CodeSmell:\n",
    "    print(f\"{smell.name} = '{smell.value}'\")\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample usage:\")\n",
    "print(f\"CodeSmell.DEAD_CODE = '{CodeSmell.DEAD_CODE.value}'\")\n",
    "print(f\"CodeSmell.FEATURE_ENVY = '{CodeSmell.FEATURE_ENVY.value}'\")\n",
    "\n",
    "# You can also look up an enum by value\n",
    "def get_smell_by_name(name):\n",
    "    for smell in CodeSmell:\n",
    "        if smell.value == name:\n",
    "            return smell\n",
    "    return None\n",
    "\n",
    "print(\"\\nLooking up by name:\")\n",
    "print(f\"get_smell_by_name('dead-code') = {get_smell_by_name('dead-code')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:01.067146Z",
     "iopub.status.busy": "2025-04-21T04:13:01.066851Z",
     "iopub.status.idle": "2025-04-21T04:13:04.768623Z",
     "shell.execute_reply": "2025-04-21T04:13:04.767719Z",
     "shell.execute_reply.started": "2025-04-21T04:13:01.067126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install langchain_community langchain-text-splitters pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:04.770413Z",
     "iopub.status.busy": "2025-04-21T04:13:04.769977Z",
     "iopub.status.idle": "2025-04-21T04:13:04.785847Z",
     "shell.execute_reply": "2025-04-21T04:13:04.785173Z",
     "shell.execute_reply.started": "2025-04-21T04:13:04.770381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "import enum\n",
    "\n",
    "# Define your structured output schema using Pydantic\n",
    "class CodeSmellSeverity(str, enum.Enum):\n",
    "    HIGH = \"HIGH\"\n",
    "    MEDIUM = \"MEDIUM\" \n",
    "    LOW = \"LOW\"\n",
    "\n",
    "class CodeSmellDetection(BaseModel):\n",
    "    smell_type: str = Field(description=\"The type of code smell detected\")\n",
    "    location: str = Field(description=\"Where in the code the smell was found\")\n",
    "    severity: CodeSmellSeverity = Field(description=\"How severe the smell is\")\n",
    "    description: str = Field(description=\"Brief explanation of the issue\")\n",
    "    refactoring_suggestion: str = Field(description=\"How to fix the code smell\")\n",
    "    code_example: Optional[str] = Field(None, description=\"Example code showing the fix\")\n",
    "\n",
    "class CodeAnalysisResult(BaseModel):\n",
    "    analysis_summary: str = Field(description=\"Overall summary of code quality\")\n",
    "    smells_detected: List[CodeSmellDetection] = Field(description=\"List of detected code smells\")\n",
    "\n",
    "# Create a parser for the structured output\n",
    "parser = PydanticOutputParser(pydantic_object=CodeAnalysisResult)\n",
    "\n",
    "# Create a prompt template that includes formatting instructions\n",
    "code_analysis_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert code analyst. Analyze the following code for code smells:\n",
    "\n",
    "```java\n",
    "{code}\n",
    "{format_instructions}\n",
    "\n",
    "Only identify code smells from this list: {valid_smells} \"\"\", input_variables=[\"code\", \"valid_smells\"], partial_variables={\"format_instructions\": parser.get_format_instructions()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:04.787042Z",
     "iopub.status.busy": "2025-04-21T04:13:04.786733Z",
     "iopub.status.idle": "2025-04-21T04:13:04.811802Z",
     "shell.execute_reply": "2025-04-21T04:13:04.811037Z",
     "shell.execute_reply.started": "2025-04-21T04:13:04.787015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_code_with_structure(code_content):\n",
    "    # Get list of valid smells for reference\n",
    "    valid_smells = \", \".join([smell.name for smell in CodeSmell])\n",
    "    \n",
    "    # Create a prompt that sends the code directly to the QA system\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze the following code for code smells:\n",
    "    \n",
    "    ```java\n",
    "    {code_content}\n",
    "    ```\n",
    "    \n",
    "    What code smells can you identify in this code and why? Only consider these code smell types: {valid_smells}.\n",
    "    \n",
    "    For each smell found, provide:\n",
    "    1. The exact smell type (from the list provided)\n",
    "    2. Location in the code (file, line numbers, method names)\n",
    "    3. Severity (HIGH, MEDIUM, LOW)\n",
    "    4. Description of why this is a code smell\n",
    "    5. Refactoring suggestion to fix the issue\n",
    "    6. Optional: Example code showing the fix\n",
    "    \n",
    "    Format your response as a structured JSON object matching this schema:\n",
    "    {parser.get_format_instructions()}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the QA system which leverages the smell knowledge base\n",
    "    try:\n",
    "        qa_result = qa.invoke(analysis_prompt)\n",
    "        \n",
    "        # Try to extract and parse JSON from the result\n",
    "        output_text = qa_result[\"result\"]\n",
    "        parsed_output = parser.parse(output_text)\n",
    "        return parsed_output\n",
    "    except Exception as e:\n",
    "        print(f\"Error in code smell analysis: {e}\")\n",
    "        print(f\"Raw QA output: {qa_result['result'] if 'qa_result' in locals() else 'No output'}\")\n",
    "        \n",
    "        # Fallback to direct LLM call if QA system parsing fails\n",
    "        try:\n",
    "            direct_output = llm.invoke(analysis_prompt)\n",
    "            parsed_output = parser.parse(direct_output.content)\n",
    "            return parsed_output\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback also failed: {e2}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:04.813138Z",
     "iopub.status.busy": "2025-04-21T04:13:04.812770Z",
     "iopub.status.idle": "2025-04-21T04:13:04.834522Z",
     "shell.execute_reply": "2025-04-21T04:13:04.833692Z",
     "shell.execute_reply.started": "2025-04-21T04:13:04.813116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_code_analysis(file_path): # Read the file content \n",
    "    with open(file_path, 'r') as f: code_content = f.read()\n",
    "        # Analyze the code\n",
    "    analysis = analyze_code_with_structure(code_content)\n",
    "\n",
    "    if not analysis:\n",
    "        print(\"Failed to analyze code.\")\n",
    "        return\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Analysis Summary: {analysis.analysis_summary}\\n\")\n",
    "    print(f\"Detected {len(analysis.smells_detected)} code smells:\")\n",
    "\n",
    "    for i, smell in enumerate(analysis.smells_detected, 1):\n",
    "        print(f\"\\n{i}. {smell.smell_type} ({smell.severity})\")\n",
    "        print(f\"   Location: {smell.location}\")\n",
    "        print(f\"   Description: {smell.description}\")\n",
    "        print(f\"   Refactoring: {smell.refactoring_suggestion}\")\n",
    "        if smell.code_example:\n",
    "            print(f\"\\n   Example fix:\\n   ```\\n{smell.code_example}\\n   ```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:04.835821Z",
     "iopub.status.busy": "2025-04-21T04:13:04.835491Z",
     "iopub.status.idle": "2025-04-21T04:13:22.027498Z",
     "shell.execute_reply": "2025-04-21T04:13:22.026693Z",
     "shell.execute_reply.started": "2025-04-21T04:13:04.835796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze a Java file from the pixel-dungeon repository\n",
    "java_file = \"pixel-dungeon/src/com/watabou/pixeldungeon/levels/HallsLevel.java\"\n",
    "display_code_analysis(java_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:22.028333Z",
     "iopub.status.busy": "2025-04-21T04:13:22.028134Z",
     "iopub.status.idle": "2025-04-21T04:13:22.049054Z",
     "shell.execute_reply": "2025-04-21T04:13:22.048051Z",
     "shell.execute_reply.started": "2025-04-21T04:13:22.028317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "import enum\n",
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 1. Define evaluation schema\n",
    "class EvaluationScore(str, enum.Enum):\n",
    "    EXCELLENT = \"EXCELLENT\"\n",
    "    GOOD = \"GOOD\"\n",
    "    ACCEPTABLE = \"ACCEPTABLE\"\n",
    "    POOR = \"POOR\"\n",
    "    INCORRECT = \"INCORRECT\"\n",
    "\n",
    "class SmellEvaluation(BaseModel):\n",
    "    detected_smell: str = Field(description=\"The detected code smell type\")\n",
    "    location: str = Field(description=\"Where the smell was detected\")\n",
    "    ground_truth_match: Optional[str] = Field(None, description=\"The matching ground truth smell if any\")\n",
    "    score: EvaluationScore = Field(description=\"Evaluation score for this detection\")\n",
    "    justification: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "class CodeSmellEvaluationResult(BaseModel):\n",
    "    overall_score: float = Field(description=\"Overall evaluation score out of 5\")\n",
    "    precision: float = Field(description=\"Ratio of correctly identified smells to all detections\")\n",
    "    recall: float = Field(description=\"Ratio of correctly identified smells to all actual smells\")\n",
    "    evaluations: List[SmellEvaluation] = Field(description=\"Individual smell evaluations\")\n",
    "    summary: str = Field(description=\"Summary of evaluation results\")\n",
    "\n",
    "# 2. Create the evaluation prompt template\n",
    "eval_template = \"\"\"\n",
    "# Instruction\n",
    "You are an expert evaluator specializing in code smell detection. Your task is to evaluate the quality of code smell detections by comparing them with ground truth data.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing code smell detection quality, which measures how accurately the system identifies:\n",
    "1. The correct type of code smell\n",
    "2. The correct location of the smell (file, line numbers, method)\n",
    "\n",
    "## Criteria\n",
    "Smell Type Accuracy: The detected smell type matches the actual smell type in the code.\n",
    "Location Accuracy: The location identified for the smell (line numbers, method, class) matches where the smell actually exists.\n",
    "Justification Quality: The explanation provided for the smell makes sense and correctly describes the issue.\n",
    "Refactoring Relevance: The suggested refactoring is appropriate for the identified smell.\n",
    "\n",
    "## Rating Rubric\n",
    "EXCELLENT: Perfect match of smell type and exact location (score: 5).\n",
    "GOOD: Correct smell type with minor location imprecision (score: 4).\n",
    "ACCEPTABLE: Partial match (either correct smell type or approximate location) (score: 3).\n",
    "POOR: Wrong smell type but area of concern correctly identified (score: 2).\n",
    "INCORRECT: Completely incorrect detection (wrong smell type and location) (score: 1).\n",
    "\n",
    "## Evaluation Steps\n",
    "STEP 1: For each detected smell, find any matching ground truth smells.\n",
    "STEP 2: Evaluate the accuracy of the detected smell type.\n",
    "STEP 3: Evaluate the precision of the location identified.\n",
    "STEP 4: Assign a score based on the rating rubric.\n",
    "STEP 5: Calculate overall precision and recall metrics.\n",
    "\n",
    "# Input Data\n",
    "## Ground Truth Smells\n",
    "{ground_truth}\n",
    "\n",
    "## Detected Smells\n",
    "{detected_smells}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# 3. Create the evaluation parser and prompt\n",
    "eval_parser = PydanticOutputParser(pydantic_object=CodeSmellEvaluationResult)\n",
    "eval_prompt = PromptTemplate(\n",
    "    template=eval_template,\n",
    "    input_variables=[\"ground_truth\", \"detected_smells\"],\n",
    "    partial_variables={\"format_instructions\": eval_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 4. Create the evaluation function\n",
    "def evaluate_smell_detection(ground_truth_data, detected_smells, llm):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of code smell detection by comparing with ground truth.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_data: Dictionary with ground truth smells\n",
    "        detected_smells: Dictionary with detected smells\n",
    "        llm: LangChain LLM instance\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results with scores and metrics\n",
    "    \"\"\"\n",
    "    # Convert data to JSON strings\n",
    "    ground_truth_str = json.dumps(ground_truth_data, indent=2)\n",
    "    detected_str = json.dumps(detected_smells, indent=2)\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = eval_prompt.format(\n",
    "        ground_truth=ground_truth_str,\n",
    "        detected_smells=detected_str\n",
    "    )\n",
    "    \n",
    "    # Get response from LLM\n",
    "    try:\n",
    "        output = llm.invoke(formatted_prompt)\n",
    "        parsed_output = eval_parser.parse(output.content)\n",
    "        return parsed_output\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        print(f\"Raw output: {output.content if 'output' in locals() else 'No output'}\")\n",
    "        return None\n",
    "\n",
    "# 5. Helper function to create ground truth dataset\n",
    "def create_ground_truth(file_path, manual_annotations):\n",
    "    \"\"\"Create ground truth data structure\"\"\"\n",
    "    return {\n",
    "        \"file_path\": file_path,\n",
    "        \"smells\": manual_annotations\n",
    "    }\n",
    "\n",
    "# 6. Main evaluation function\n",
    "def evaluate_code_analysis(file_path, manual_annotations, llm):\n",
    "    \"\"\"Run a full evaluation on a code file\"\"\"\n",
    "    # Get the code content\n",
    "    with open(file_path, 'r') as f:\n",
    "        code_content = f.read()\n",
    "    \n",
    "    # Run code smell detection\n",
    "    analysis_result = analyze_code_with_structure(code_content)\n",
    "    \n",
    "    if not analysis_result:\n",
    "        print(\"Failed to analyze code\")\n",
    "        return None\n",
    "    \n",
    "    # Format detected smells\n",
    "    detected_smells = {\n",
    "        \"file_path\": file_path,\n",
    "        \"smells\": []\n",
    "    }\n",
    "    \n",
    "    for smell in analysis_result.smells_detected:\n",
    "        detected_smells[\"smells\"].append({\n",
    "            \"smell_type\": smell.smell_type,\n",
    "            \"location\": smell.location,\n",
    "            \"description\": smell.description,\n",
    "            \"severity\": str(smell.severity),\n",
    "            \"refactoring\": smell.refactoring_suggestion\n",
    "        })\n",
    "    \n",
    "    # Create ground truth\n",
    "    ground_truth = create_ground_truth(file_path, manual_annotations)\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluation = evaluate_smell_detection(ground_truth, detected_smells, llm)\n",
    "    \n",
    "    # Display results\n",
    "    if evaluation:\n",
    "        print(f\"Overall Score: {evaluation.overall_score:.2f}/5.0\")\n",
    "        print(f\"Precision: {evaluation.precision:.2f}\")\n",
    "        print(f\"Recall: {evaluation.recall:.2f}\")\n",
    "        print(f\"\\nSummary: {evaluation.summary}\\n\")\n",
    "        \n",
    "        print(\"Individual Evaluations:\")\n",
    "        for i, eval_item in enumerate(evaluation.evaluations, 1):\n",
    "            print(f\"\\n{i}. {eval_item.detected_smell} ({eval_item.location})\")\n",
    "            print(f\"   Score: {eval_item.score.value}\")\n",
    "            print(f\"   Matched with: {eval_item.ground_truth_match if eval_item.ground_truth_match else 'No match'}\")\n",
    "            print(f\"   Justification: {eval_item.justification}\")\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:22.050271Z",
     "iopub.status.busy": "2025-04-21T04:13:22.049962Z",
     "iopub.status.idle": "2025-04-21T04:13:43.942881Z",
     "shell.execute_reply": "2025-04-21T04:13:43.942131Z",
     "shell.execute_reply.started": "2025-04-21T04:13:22.050245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example ground truth data for a file - here we can utilize data from static analyzers such as Sonarqube etc.\n",
    "evaluation_files = {\n",
    "    \"pixel-dungeon/src/com/watabou/pixeldungeon/levels/HallsLevel.java\": [\n",
    "        {\n",
    "            \"smell_type\": \"MAGIC_NUMBER\",\n",
    "            \"location\": \"multiple locations: lines 25-26, 37, 52-53, 85, 89\",\n",
    "            \"severity\": \"MEDIUM\"\n",
    "        },\n",
    "        {\n",
    "            \"smell_type\": \"LONG_METHOD\",\n",
    "            \"location\": \"decorate() method, lines 74-101\",\n",
    "            \"severity\": \"MEDIUM\"\n",
    "        },\n",
    "        {\n",
    "            \"smell_type\": \"CONDITIONAL_COMPLEXITY\",\n",
    "            \"location\": \"decorate() method, lines 76-89\",\n",
    "            \"severity\": \"LOW\"\n",
    "        },\n",
    "        {\n",
    "            \"smell_type\": \"DUPLICATED_CODE\",\n",
    "            \"location\": \"tileName() and tileDesc() methods, lines 104-138\",\n",
    "            \"severity\": \"LOW\"\n",
    "        },\n",
    "        {\n",
    "            \"smell_type\": \"DEAD_CODE\",\n",
    "            \"location\": \"map[i] == 63 condition in addVisuals method, line 151\",\n",
    "            \"severity\": \"MEDIUM\"\n",
    "        },\n",
    "        {\n",
    "            \"smell_type\": \"FEATURE_ENVY\",\n",
    "            \"location\": \"addVisuals method, lines 149-153\",\n",
    "            \"severity\": \"LOW\"\n",
    "        },\n",
    "        {\n",
    "            \"smell_type\": \"PRIMITIVE_OBSESSION\",\n",
    "            \"location\": \"throughout class, using boolean arrays and ints for terrain\",\n",
    "            \"severity\": \"LOW\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run evaluation on a single file\n",
    "java_file = \"pixel-dungeon/src/com/watabou/pixeldungeon/levels/HallsLevel.java\"\n",
    "evaluation = evaluate_code_analysis(java_file, evaluation_files, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:13:43.944167Z",
     "iopub.status.busy": "2025-04-21T04:13:43.943699Z",
     "iopub.status.idle": "2025-04-21T04:14:11.397658Z",
     "shell.execute_reply": "2025-04-21T04:14:11.396829Z",
     "shell.execute_reply.started": "2025-04-21T04:13:43.944144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run evaluations and collect results\n",
    "results = {}\n",
    "for file_path, annotations in evaluation_files.items():\n",
    "    print(f\"\\n=== Evaluating {file_path} ===\")\n",
    "    eval_result = evaluate_code_analysis(file_path, annotations, llm)\n",
    "    if eval_result:\n",
    "        results[file_path] = eval_result\n",
    "\n",
    "# Calculate overall metrics\n",
    "if results:\n",
    "    total_score = sum(result.overall_score for result in results.values())\n",
    "    avg_score = total_score / len(results)\n",
    "    avg_precision = sum(result.precision for result in results.values()) / len(results)\n",
    "    avg_recall = sum(result.recall for result in results.values()) / len(results)\n",
    "\n",
    "    print(\"\\n=== OVERALL EVALUATION ===\")\n",
    "    print(f\"Average Score: {avg_score:.2f}/5.0\")\n",
    "    print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
